Recommended Notebook Skeleton Setup Install libs, set seeds, pick model_name, paths. Data load Load PersianQA. Immediately split train → (train, val) if a dev split isn’t given. EDA (train-only) Lengths (context/question/answer), class balance (answerable vs unanswerable), language quirks (ZWNJ, punctuation). Do not peek at test for EDA. Text normalization decisions (frozen before training) E.g., Persian ZWNJ cleaner, whitespace normalization, punctuation rules. Decide once (based on train/your prior knowledge), then apply the same function to val/test/ParsiNLU. Don’t “fit” cleaners on test (no statistics collected from test). Tokenization (same config for all splits) Choose max_length, stride (for long contexts), truncation policy. You configure tokenization on train, then reuse exactly the same for val/test/ParsiNLU. Tokenizer is pre-trained (no “fitting”), so no leakage risk, but keep params identical. Dataloaders Build for train/val. Keep test loader ready but don’t run it yet. Model setup Load pre-trained (e.g., ParsBERT/XLM-R), QA head. Training Train on train split only. Validation & hyper-parameter tuning Evaluate on val each epoch. Tune LR, epochs, max_length/stride, and no-answer threshold (if applicable) using val only. Lock config Once you settle the best config/checkpoint on val, freeze everything. Final evaluation (one-time) Run on PersianQA test. Save predictions, F1/EM, and a CSV/JSON of errors. External generalization (optional but strong) Adapt ParsiNLU-QA format → run exact same prediction & metric pipeline. Report zero-shot scores (and, if allowed, a second experiment fine-tuning on ParsiNLU train, clearly separated). Error analysis & ablations (adds resume value) Buckets: context length, answer type (name/date/location), unanswerables, threshold sweeps. Show top FP/FN examples. Reproducibility One cell that runs eval end-to-end; log versions; save seeds/configs.
