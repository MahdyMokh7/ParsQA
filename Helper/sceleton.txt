## ðŸ“Œ Project Roadmap: PersianQA Fine-Tuning

1. **Load Data âœ…**  
   - Implemented a robust loader (`load_persianqa`)  
   - Handles local JSONs, Drive cache, Hugging Face Hub, or GitHub clone  

2. **Exploratory Data Analysis (EDA) âœ…**  
   - Basic dataset overview  
   - Core statistics (context/question lengths, answers)  
   - Integrity checks (span mismatches, empty answers)  
   - Length distributions  
   - Duplicate & leakage detection  

3. **Data Cleaning & Preprocessing**  
   - Remove/fix span mismatch examples  
   - Handle duplicates or leaks across splits  
   - Normalize text (optional: remove extra whitespace/diacritics)  

4. **Tokenization & Feature Preparation**  
   - Use `AutoTokenizer` (ParsBERT / XLM-R)  
   - Convert Q + context into tokenized inputs  
   - Align answer spans with token positions  

5. **Model Setup**  
   - Load pretrained QA model (`AutoModelForQuestionAnswering`)  
   - Choose training hyperparameters (lr, epochs, batch size, max_length, stride)  

6. **Training**  
   - Fine-tune ParsBERT  
   - Fine-tune XLM-R  
   - Save checkpoints  

7. **Evaluation**  
   - Compute EM (Exact Match) and F1 on validation/test split  
   - Compare ParsBERT vs XLM-R  

8. **Analysis & Reporting**  
   - Summarize results (tables/plots)  
   - Discuss performance differences  
   - Identify error cases (e.g., long contexts, ambiguous answers)  

9. **Optional Extensions**  
   - Hyperparameter tuning (grid search or manual sweeps)  
   - Experiment with data augmentation  
   - Push trained models to Hugging Face Hub  
